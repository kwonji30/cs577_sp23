{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4444cf9-abdd-4e62-a134-2c7ce34974b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ec75c2-7708-430f-a874-8ddcf8c0f781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff5cba7-4506-4bd0-b1fe-edb3e5850687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871ad66c-4be3-4500-8fb7-fac586a77b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) <= 65:\n",
    "            padded_seq = torch.nn.functional.pad(seq, (0, 0, 0, 65 - seq.size(0)), mode='constant', value=0)\n",
    "        else:\n",
    "            print(sequences.numel())\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2b8228-5315-40a5-8e26-70ad490ba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.model = AutoModel.from_pretrained('gpt2').to(device)\n",
    "        self.data = pd.read_csv(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "        gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "        padded_outputs = pad_sequences(gpt_outputs)\n",
    "        return (padded_outputs, torch.tensor(self.data['Label'].iloc[idx], dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859f2223-1cb4-44a8-80ce-68eb17f3a019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = customDataset(\"new_data.csv\")\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=data,\n",
    "                                              batch_size=32,\n",
    "                                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7535554b-5867-4a41-b179-c9bd969a6ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model = DNN(input_size=768*65, hidden_size=52, num_classes=1).to(device)\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(gpt_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332989c4-0c54-4596-89ce-7efede494185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.6934838891029358, Train Acc: 56.601331360946745\n",
      "Epoch: 1 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.634870171546936, Train Acc: 63.99778106508876\n",
      "Epoch: 2 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.5645986199378967, Train Acc: 69.8224852071006\n",
      "Epoch: 3 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4749314785003662, Train Acc: 77.1819526627219\n",
      "Epoch: 4 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3949655294418335, Train Acc: 82.3594674556213\n",
      "Epoch: 5 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.42069733142852783, Train Acc: 80.45488165680473\n",
      "Epoch: 6 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.324857622385025, Train Acc: 85.70636094674556\n",
      "Epoch: 7 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.2626688480377197, Train Acc: 89.42307692307692\n",
      "Epoch: 8 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.2818368673324585, Train Acc: 88.09171597633136\n",
      "Epoch: 9 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4335102438926697, Train Acc: 77.2189349112426\n",
      "Epoch: 10 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3279739022254944, Train Acc: 86.48298816568047\n",
      "Epoch: 11 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.25826701521873474, Train Acc: 89.82988165680473\n",
      "Epoch: 12 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.20079736411571503, Train Acc: 92.3076923076923\n",
      "Epoch: 13 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1791241616010666, Train Acc: 93.3801775147929\n",
      "Epoch: 14 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.2107599526643753, Train Acc: 91.71597633136095\n",
      "Epoch: 15 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3244021236896515, Train Acc: 86.26109467455622\n",
      "Epoch: 16 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.29948878288269043, Train Acc: 87.20414201183432\n",
      "Epoch: 17 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.21868379414081573, Train Acc: 91.67899408284023\n",
      "Epoch: 18 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.23389819264411926, Train Acc: 90.34763313609467\n",
      "Epoch: 19 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.16700100898742676, Train Acc: 93.71301775147928\n",
      "Epoch: 20 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.13811175525188446, Train Acc: 95.0258875739645\n",
      "Epoch: 21 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.13372567296028137, Train Acc: 95.08136094674556\n",
      "Epoch: 22 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.14439646899700165, Train Acc: 94.56360946745562\n",
      "Epoch: 23 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.2131057232618332, Train Acc: 91.49408284023669\n",
      "Epoch: 24 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.34511545300483704, Train Acc: 84.72633136094674\n",
      "Epoch: 25 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1896207332611084, Train Acc: 91.84541420118343\n",
      "Epoch: 26 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.11896481364965439, Train Acc: 95.41420118343196\n",
      "Epoch: 27 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.11292824894189835, Train Acc: 95.82100591715977\n",
      "Epoch: 28 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.11628282815217972, Train Acc: 95.26627218934911\n",
      "Epoch: 29 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.17768652737140656, Train Acc: 92.6775147928994\n",
      "Epoch: 30 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1679655760526657, Train Acc: 93.28772189349112\n",
      "Epoch: 31 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.13763459026813507, Train Acc: 94.32322485207101\n",
      "Epoch: 32 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.09330542385578156, Train Acc: 96.72707100591715\n",
      "Epoch: 33 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.08276575058698654, Train Acc: 97.37426035502959\n",
      "Epoch: 34 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.09621628373861313, Train Acc: 96.61612426035504\n",
      "Epoch: 35 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.14447905123233795, Train Acc: 94.78550295857988\n",
      "Epoch: 36 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.16051286458969116, Train Acc: 94.32322485207101\n",
      "Epoch: 37 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.2453659325838089, Train Acc: 91.25369822485207\n",
      "Epoch: 38 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1502881646156311, Train Acc: 94.74852071005917\n",
      "Epoch: 39 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.12292543798685074, Train Acc: 95.39571005917159\n",
      "Epoch: 40 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1069469004869461, Train Acc: 95.98742603550296\n",
      "Epoch: 41 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.11747085303068161, Train Acc: 95.5991124260355\n",
      "Epoch: 42 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.1443011462688446, Train Acc: 94.56360946745562\n",
      "Epoch: 43 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.27116069197654724, Train Acc: 89.12721893491124\n",
      "Epoch: 44 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.10882440954446793, Train Acc: 96.11686390532545\n",
      "Epoch: 45 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.16905102133750916, Train Acc: 94.04585798816568\n",
      "Epoch: 46 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.15165746212005615, Train Acc: 94.45266272189349\n",
      "Epoch: 47 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.18512825667858124, Train Acc: 92.60355029585799\n",
      "Epoch: 48 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.12260890752077103, Train Acc: 95.13683431952663\n",
      "Epoch: 49 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.0644618570804596, Train Acc: 98.11390532544378\n",
      "Epoch: 50 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.06100822240114212, Train Acc: 98.24334319526628\n",
      "Epoch: 51 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.061034850776195526, Train Acc: 97.91050295857988\n",
      "Epoch: 52 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07553054392337799, Train Acc: 97.46671597633136\n",
      "Epoch: 53 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.056752271950244904, Train Acc: 98.1508875739645\n",
      "Epoch: 54 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07532615214586258, Train Acc: 97.2078402366864\n",
      "Epoch: 55 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07102452963590622, Train Acc: 97.67011834319527\n",
      "Epoch: 56 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.05508548021316528, Train Acc: 98.26183431952663\n",
      "Epoch: 57 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.049743521958589554, Train Acc: 98.39127218934911\n",
      "Epoch: 58 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.051946740597486496, Train Acc: 98.48372781065089\n",
      "Epoch: 59 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04499952495098114, Train Acc: 98.66863905325444\n",
      "Epoch: 60 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03952403739094734, Train Acc: 98.8905325443787\n",
      "Epoch: 61 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.038890108466148376, Train Acc: 98.87204142011835\n",
      "Epoch: 62 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.037990752607584, Train Acc: 98.96449704142012\n",
      "Epoch: 63 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04122466593980789, Train Acc: 98.7241124260355\n",
      "Epoch: 64 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04490940272808075, Train Acc: 98.63165680473372\n",
      "Epoch: 65 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03974808380007744, Train Acc: 98.76109467455622\n",
      "Epoch: 66 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.044760797172784805, Train Acc: 98.52071005917159\n",
      "Epoch: 67 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.041839756071567535, Train Acc: 98.76109467455622\n",
      "Epoch: 68 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.05421455577015877, Train Acc: 98.22485207100591\n",
      "Epoch: 69 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03753119707107544, Train Acc: 98.81656804733728\n",
      "Epoch: 70 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03791671246290207, Train Acc: 98.85355029585799\n",
      "Epoch: 71 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03703327104449272, Train Acc: 98.87204142011835\n",
      "Epoch: 72 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.038696132600307465, Train Acc: 98.83505917159763\n",
      "Epoch: 73 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07826503366231918, Train Acc: 97.28180473372781\n",
      "Epoch: 74 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04308820515871048, Train Acc: 98.63165680473372\n",
      "Epoch: 75 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04022969305515289, Train Acc: 98.8905325443787\n",
      "Epoch: 76 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03480035811662674, Train Acc: 98.98298816568047\n",
      "Epoch: 77 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03432989865541458, Train Acc: 98.94600591715977\n",
      "Epoch: 78 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.05244046822190285, Train Acc: 98.18786982248521\n",
      "Epoch: 79 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04353274405002594, Train Acc: 98.61316568047337\n",
      "Epoch: 80 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.04874517768621445, Train Acc: 98.33579881656804\n",
      "Epoch: 81 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03839024528861046, Train Acc: 98.87204142011835\n",
      "Epoch: 82 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.032771892845630646, Train Acc: 98.96449704142012\n",
      "Epoch: 83 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03211686387658119, Train Acc: 99.0569526627219\n",
      "Epoch: 84 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03076796606183052, Train Acc: 99.00147928994083\n",
      "Epoch: 85 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.030011164024472237, Train Acc: 99.00147928994083\n",
      "Epoch: 86 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.02988438867032528, Train Acc: 99.07544378698225\n",
      "Epoch: 87 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.029488420113921165, Train Acc: 98.98298816568047\n",
      "Epoch: 88 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.0296199768781662, Train Acc: 99.00147928994083\n",
      "Epoch: 89 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.029600242152810097, Train Acc: 99.01997041420118\n",
      "Epoch: 90 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07135895639657974, Train Acc: 97.2448224852071\n",
      "Epoch: 91 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.07115895301103592, Train Acc: 97.9474852071006\n",
      "Epoch: 92 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.0337325856089592, Train Acc: 99.57470414201184\n",
      "Epoch: 93 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03050488792359829, Train Acc: 99.61168639053254\n",
      "Epoch: 94 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.030082637444138527, Train Acc: 99.68565088757397\n",
      "Epoch: 95 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.0304549653083086, Train Acc: 99.70414201183432\n",
      "Epoch: 96 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.029542624950408936, Train Acc: 99.70414201183432\n",
      "Epoch: 97 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.041152190417051315, Train Acc: 98.85355029585799\n",
      "Epoch: 98 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.03673884645104408, Train Acc: 98.81656804733728\n",
      "Epoch: 99 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.05820590257644653, Train Acc: 97.92899408284023\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch} \\n =====================\")\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "        gpt_model.train()\n",
    "        # Forward Pass\n",
    "        logits = gpt_model(flattened_inputs).squeeze()\n",
    "        rounded_labels = torch.round(labels)\n",
    "        pred = torch.round(torch.sigmoid(logits))\n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(rounded_labels, pred)\n",
    "        # Zero the graident\n",
    "        optimizer.zero_grad()\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "        # Perform gradient descent\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    print(f\"\\nTrain Loss: {train_loss}, Train Acc: {train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc4d339-8db3-4947-9a78-3b3e89c06efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gpt_model.state_dict(), 'student_model_wic_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285acd10-4893-43d2-9697-2c80388b9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df_data = pd.read_csv(path+\"data.txt\",\n",
    "                              delimiter='\\t',\n",
    "                              names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "        df_label = pd.read_csv(path+'gold.txt',\n",
    "                               delimiter='\\t',\n",
    "                               names=['label'])\n",
    "        self.data = pd.concat([df_data, df_label], axis=1)\n",
    "        self.data['Joined'] = self.data['Context1'] + \" \" + self.data['Context2']\n",
    "        self.data['label'] = self.data['label'].map(lambda x: 0 if x == 'F' else 1)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.model = AutoModel.from_pretrained('gpt2').to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "        gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "        padded_outputs = pad_sequences(gpt_outputs)\n",
    "        return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \n",
    "        \n",
    "test_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\test\\test.\"\n",
    "test_data = testDataset(test_path)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                             batch_size=32,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "195ceeb5-c707-47d9-bae5-c31b898a159a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN(input_size=768*65, hidden_size=52, num_classes=1).to(device)\n",
    "model.load_state_dict(torch.load('student_model_wic_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b14568a-1257-4a02-9593-ae985cf62b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list, label_list = [], []\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "        test_logits = model(flattened_inputs)\n",
    "        pred = torch.round(torch.sigmoid(test_logits))\n",
    "        pred_list.append(pred.cpu().numpy())\n",
    "        label_list.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9a6deed-f010-4474-bbfd-074f2491d775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_array = np.concatenate(pred_list)\n",
    "label_array = np.concatenate(label_list)\n",
    "pred_array  = pred_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62238804-3d31-4aec-905d-7af374b3cefc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5328571428571428"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_array == label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83a88a96-162d-43e3-92da-a27c8fae354b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cde280a1-b889-476e-b128-959e7468d6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb5a53-8dee-4ee9-b7f6-08338b71ff92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
