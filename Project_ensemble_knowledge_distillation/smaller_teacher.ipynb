{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e801bfd-1f03-4015-b0ad-14efb195111c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joowa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "glove = api.load('glove-wiki-gigaword-50')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bafe91-b844-4dab-9abe-1b4c6a1557ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, num_epochs, device, accuracy_fn):\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch} \\n ==========\")\n",
    "        ### Training\n",
    "        train_loss, train_acc = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "            model.train()\n",
    "            # Forard Pass\n",
    "            logits = model(flattened_inputs).squeeze()\n",
    "            pred = torch.round(torch.sigmoid(logits))\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            train_loss += loss\n",
    "            train_acc += accuracy_fn(labels, pred)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Perform gradient descent\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        ### Testing\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inptus, labels = inputs.to(device), labels.to(device)\n",
    "                flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "                # Forward pass\n",
    "                test_logits = model(flattened_inputs).squeeze()\n",
    "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "                # Calculate the loss and accuracy\n",
    "                test_loss += criterion(test_logits, labels)\n",
    "                test_acc += accuracy_fn(labels, test_pred)\n",
    "            test_loss /= len(valid_loader)\n",
    "            test_acc /= len(valid_loader)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}% | Test loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9018e209-2ec3-4f5f-a8fb-3ca5668e0c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_glove(model, optimizer, criterion, train_loader, valid_loader, num_epochs, device, accuracy_fn):\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch} \\n ==========\")\n",
    "        ### Training\n",
    "        train_loss, train_acc = 0, 0\n",
    "        for inputs, labels, seq_lengths in train_loader:\n",
    "            inputs, labels, seq_lengths = inputs.to(device), labels.to(device), seq_lengths.to(device)\n",
    "            model.train()\n",
    "            # Forard Pass\n",
    "            logits = model(inputs, seq_lengths).squeeze()\n",
    "            pred = torch.round(torch.sigmoid(logits))\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            train_loss += loss\n",
    "            train_acc += accuracy_fn(labels, pred)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Perform gradient descent\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        ### Testing\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels, seq_lengths in valid_loader:\n",
    "                inptus, labels, seq_lengths = inputs.to(device), labels.to(device), seq_lengths.to(device)\n",
    "                # Forward pass\n",
    "                test_logits = model(inputs, seq_lengths).squeeze()\n",
    "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "                # Calculate the loss and accuracy\n",
    "                test_loss += criterion(test_logits, labels)\n",
    "                test_acc += accuracy_fn(labels, test_pred)\n",
    "            test_loss /= len(valid_loader)\n",
    "            test_acc /= len(valid_loader)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}% | Test loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6795cdd2-18ec-453e-805c-ff1e10f8ea68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c755a4a7-354f-4be2-8e22-cef128bfd518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    # pad the inputs with zeros to make them the same length\n",
    "    inputs_padded = rnn_utils.pad_sequence(inputs, batch_first=True)\n",
    "    # get the sequence lenghts of the inputs\n",
    "    seq_length = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    \n",
    "    # sort the inputs and labels by the sequence lengths\n",
    "    seq_length, sort_idx = seq_length.sort(descending=True)\n",
    "    inputs_padded = inputs_padded[sort_idx].to(device)\n",
    "    labels_sorted = torch.tensor(labels, dtype=torch.float32)[sort_idx].to(device)\n",
    "\n",
    "    return inputs_padded, labels_sorted, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84abde47-1ead-4c10-beaa-45bab3ff7996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Divides a list into sublists with an equal amount of items in each sublist.\"\"\"\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fc0d9f-00f5-4859-b4cd-06043110ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_maxLength(sentence_list, tokenizer):\n",
    "    length_list = []\n",
    "    sentence_lists = chunk_list(sentence_list, 200)\n",
    "    for block in sentence_lists:\n",
    "        torch.cuda.empty_cache()\n",
    "        token = tokenizer(block,\n",
    "                          padding=True,\n",
    "                          return_tensors='pt')\n",
    "        length_list.append(token['input_ids'].shape[1])\n",
    "    return max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af147be-789d-43a1-ab9b-4f08e29982c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) <= 65:\n",
    "            padded_seq = torch.nn.functional.pad(seq, (0, 0, 0, 65 - seq.size(0)), mode='constant', value=0)\n",
    "        else:\n",
    "            print(sequences.numel())\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89aa6956-927a-4604-8566-825c17f5c65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "class WiCDataset(Dataset):\n",
    "    def __init__(self, path, mode):\n",
    "        self.mode = mode\n",
    "        if mode == \"gpt\":\n",
    "            self.mode = 'gpt2'\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "        elif mode == \"bert\":\n",
    "            self.mode = 'bert-base-uncased'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.mode) \n",
    "            self.model = AutoModel.from_pretrained(self.mode).to(device)\n",
    "\n",
    "        df_data = pd.read_csv(path+\"data.txt\",\n",
    "                              delimiter='\\t',\n",
    "                              names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "        df_label = pd.read_csv(path+'gold.txt',\n",
    "                               delimiter='\\t',\n",
    "                               names=['label'])\n",
    "        self.data = pd.concat([df_data, df_label], axis=1)\n",
    "        self.data['Joined'] = self.data['Context1'] + \" \" + self.data['Context2']\n",
    "        self.data['label'] = self.data['label'].map(lambda x: 0 if x == 'F' else 1)\n",
    "        #self.maxLength = find_maxLength(self.data['Joined'].tolist(), self.tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'gpt2':\n",
    "           # self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "           # gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "            padded_outputs = pad_sequences(gpt_outputs)\n",
    "           # with torch.inference_mode():\n",
    "           #     gpt_outputs = self.model(**gpt_token)\n",
    "            return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \n",
    "            \n",
    "            # sentence_lists = chunk_list(self.data['Joined'].tolist(), 200)\n",
    "            # tensor_list = []\n",
    "            # for block in sentence_lists:\n",
    "            #     torch.cuda.empty_cache()\n",
    "            #     gpt_token = self.tokenizer(block, padding='max_length', return_tensors='pt', max_length=65).to(device)\n",
    "            #     with torch.inference_mode():\n",
    "            #         gpt_outputs = self.model(**gpt_token)\n",
    "            #     tensor_list.append(gpt_outputs[0])\n",
    "            # gpt_tensor = torch.cat(tensor_list, dim = 0)\n",
    "            # return (gpt_tensor[idx].cpu(), torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long))\n",
    "        \n",
    "        \n",
    "        elif self.mode == 'bert-base-uncased':\n",
    "            bert_token = self.tokenizer(self.data['Joined'].iloc[idx], padding='max_length', return_tensors='pt', max_length=68).to(device)        \n",
    "            with torch.inference_mode():\n",
    "                bert_outputs = self.model(**bert_token)\n",
    "            return (bert_outputs[0], torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32))\n",
    "        \n",
    "            \n",
    "        elif self.mode == 'glove':\n",
    "            row = self.data.iloc[idx]\n",
    "            words = word_tokenize(row.Joined.lower())\n",
    "\n",
    "            indices = [glove.get_index(w) for w in words if glove.has_index_for(w)]\n",
    "            indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "            return indices_tensor, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "train_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\train\\train.\"\n",
    "valid_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\dev\\dev.\"\n",
    "test_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\test\\test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613e90c-7e12-4f20-bec4-9ea7a6f9d959",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc1cefb-521e-4162-9da5-8c68d0915a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = WiCDataset(train_path, \"glove\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "valid_data = WiCDataset(valid_path, \"glove\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "test_data = WiCDataset(test_path, \"glove\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd38dfc2-b0bc-49e4-a03d-b010c12193a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.FloatTensor(glove.vectors))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers,\n",
    "                          bidirectional = True,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, seq, seq_length):\n",
    "        inputs_embedded = self.emb(seq)\n",
    "        seq_length = seq_length.cpu()\n",
    "        packed_input = rnn_utils.pack_padded_sequence(inputs_embedded, seq_length, batch_first=True)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), seq_length - 1, :self.hidden_dim]\n",
    "        out_reverse = output[:, 0, self.hidden_dim:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        output = self.fc(out_reduced)\n",
    "        return output\n",
    "\n",
    "glove_model = LSTM(50, 128, 1, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908920a-3996-4c5b-bd6e-77d6d2a799e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(glove_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80434739-dfcb-4d9b-ae59-4dac613af8a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0549, Train Acc: 98.1509% | Test loss: 2.3807, Test Acc: 54.1118%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0406, Train Acc: 98.6317% | Test loss: 2.6156, Test Acc: 52.1382%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0468, Train Acc: 98.4837% | Test loss: 2.4443, Test Acc: 51.8092%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0369, Train Acc: 98.8166% | Test loss: 2.6459, Test Acc: 53.7829%\n",
      "Epoch: 4 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0337, Train Acc: 98.8905% | Test loss: 2.7209, Test Acc: 54.7697%\n",
      "Epoch: 5 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0333, Train Acc: 98.9460% | Test loss: 2.5958, Test Acc: 53.6184%\n",
      "Epoch: 6 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0309, Train Acc: 99.1309% | Test loss: 2.9188, Test Acc: 51.8092%\n",
      "Epoch: 7 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0227, Train Acc: 99.3528% | Test loss: 2.8814, Test Acc: 53.4539%\n",
      "Epoch: 8 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0192, Train Acc: 99.4822% | Test loss: 2.8821, Test Acc: 54.1118%\n",
      "Epoch: 9 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0101, Train Acc: 99.7411% | Test loss: 3.0779, Test Acc: 53.6184%\n",
      "Epoch: 10 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0100, Train Acc: 99.7411% | Test loss: 3.1492, Test Acc: 55.0987%\n",
      "Epoch: 11 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0086, Train Acc: 99.8336% | Test loss: 3.2914, Test Acc: 55.5921%\n",
      "Epoch: 12 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0158, Train Acc: 99.6487% | Test loss: 3.1281, Test Acc: 51.6447%\n",
      "Epoch: 13 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0576, Train Acc: 97.8920% | Test loss: 2.6821, Test Acc: 54.6053%\n",
      "Epoch: 14 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0398, Train Acc: 98.6132% | Test loss: 2.5901, Test Acc: 54.1118%\n",
      "Epoch: 15 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0178, Train Acc: 99.5562% | Test loss: 2.7512, Test Acc: 53.4539%\n",
      "Epoch: 16 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0077, Train Acc: 99.7966% | Test loss: 2.9299, Test Acc: 54.9342%\n",
      "Epoch: 17 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0063, Train Acc: 99.9075% | Test loss: 2.9570, Test Acc: 54.9342%\n",
      "Epoch: 18 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0057, Train Acc: 99.8336% | Test loss: 3.0853, Test Acc: 55.2632%\n",
      "Epoch: 19 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0060, Train Acc: 99.8521% | Test loss: 3.2133, Test Acc: 54.9342%\n",
      "Epoch: 20 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0264, Train Acc: 99.2604% | Test loss: 2.9821, Test Acc: 54.1118%\n",
      "Epoch: 21 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0317, Train Acc: 98.9830% | Test loss: 2.9324, Test Acc: 54.2763%\n",
      "Epoch: 22 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0196, Train Acc: 99.3898% | Test loss: 2.9444, Test Acc: 54.6053%\n",
      "Epoch: 23 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0078, Train Acc: 99.7966% | Test loss: 3.0229, Test Acc: 55.7566%\n",
      "Epoch: 24 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0053, Train Acc: 99.9445% | Test loss: 3.0416, Test Acc: 55.4276%\n",
      "Epoch: 25 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0064, Train Acc: 99.9075% | Test loss: 3.1762, Test Acc: 53.7829%\n",
      "Epoch: 26 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0012, Train Acc: 99.9815% | Test loss: 3.3046, Test Acc: 53.9474%\n",
      "Epoch: 27 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0004, Train Acc: 100.0000% | Test loss: 3.3653, Test Acc: 54.1118%\n",
      "Epoch: 28 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0003, Train Acc: 100.0000% | Test loss: 3.4134, Test Acc: 54.2763%\n",
      "Epoch: 29 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0002, Train Acc: 100.0000% | Test loss: 3.4558, Test Acc: 54.2763%\n",
      "Epoch: 30 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0002, Train Acc: 100.0000% | Test loss: 3.4945, Test Acc: 54.4408%\n",
      "Epoch: 31 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5307, Test Acc: 54.9342%\n",
      "Epoch: 32 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5647, Test Acc: 54.9342%\n",
      "Epoch: 33 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5973, Test Acc: 54.7697%\n",
      "Epoch: 34 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6287, Test Acc: 54.9342%\n",
      "Epoch: 35 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6588, Test Acc: 54.7697%\n",
      "Epoch: 36 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6883, Test Acc: 54.6053%\n",
      "Epoch: 37 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7171, Test Acc: 54.4408%\n",
      "Epoch: 38 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7453, Test Acc: 54.4408%\n",
      "Epoch: 39 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7730, Test Acc: 54.4408%\n",
      "Epoch: 40 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8002, Test Acc: 54.4408%\n",
      "Epoch: 41 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8273, Test Acc: 54.4408%\n",
      "Epoch: 42 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8540, Test Acc: 54.4408%\n",
      "Epoch: 43 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8802, Test Acc: 54.1118%\n",
      "Epoch: 44 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9062, Test Acc: 54.1118%\n",
      "Epoch: 45 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9320, Test Acc: 54.2763%\n",
      "Epoch: 46 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9577, Test Acc: 54.2763%\n",
      "Epoch: 47 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9833, Test Acc: 54.1118%\n",
      "Epoch: 48 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0087, Test Acc: 54.2763%\n",
      "Epoch: 49 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0340, Test Acc: 54.2763%\n",
      "Epoch: 50 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0592, Test Acc: 54.2763%\n",
      "Epoch: 51 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0843, Test Acc: 54.2763%\n",
      "Epoch: 52 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1094, Test Acc: 54.2763%\n",
      "Epoch: 53 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1345, Test Acc: 54.2763%\n",
      "Epoch: 54 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1594, Test Acc: 54.1118%\n",
      "Epoch: 55 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1844, Test Acc: 54.2763%\n",
      "Epoch: 56 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2090, Test Acc: 54.2763%\n",
      "Epoch: 57 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2341, Test Acc: 54.2763%\n",
      "Epoch: 58 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2589, Test Acc: 54.2763%\n",
      "Epoch: 59 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2839, Test Acc: 54.2763%\n",
      "Epoch: 60 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3087, Test Acc: 54.2763%\n",
      "Epoch: 61 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3336, Test Acc: 54.1118%\n",
      "Epoch: 62 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3585, Test Acc: 54.1118%\n",
      "Epoch: 63 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3833, Test Acc: 54.1118%\n",
      "Epoch: 64 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4084, Test Acc: 54.1118%\n",
      "Epoch: 65 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4333, Test Acc: 54.1118%\n",
      "Epoch: 66 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4583, Test Acc: 53.9474%\n",
      "Epoch: 67 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4832, Test Acc: 53.9474%\n",
      "Epoch: 68 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5082, Test Acc: 53.9474%\n",
      "Epoch: 69 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5331, Test Acc: 53.9474%\n",
      "Epoch: 70 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5582, Test Acc: 53.9474%\n",
      "Epoch: 71 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5833, Test Acc: 53.9474%\n",
      "Epoch: 72 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6084, Test Acc: 54.1118%\n",
      "Epoch: 73 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6334, Test Acc: 54.1118%\n",
      "Epoch: 74 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6586, Test Acc: 53.9474%\n",
      "Epoch: 75 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6838, Test Acc: 53.9474%\n",
      "Epoch: 76 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7089, Test Acc: 53.9474%\n",
      "Epoch: 77 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7339, Test Acc: 53.9474%\n",
      "Epoch: 78 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7592, Test Acc: 53.7829%\n",
      "Epoch: 79 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7844, Test Acc: 53.6184%\n",
      "Epoch: 80 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8095, Test Acc: 53.6184%\n",
      "Epoch: 81 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8348, Test Acc: 53.6184%\n",
      "Epoch: 82 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8600, Test Acc: 53.6184%\n",
      "Epoch: 83 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8850, Test Acc: 53.6184%\n",
      "Epoch: 84 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9102, Test Acc: 53.6184%\n",
      "Epoch: 85 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9354, Test Acc: 53.6184%\n",
      "Epoch: 86 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9603, Test Acc: 53.4539%\n",
      "Epoch: 87 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9856, Test Acc: 53.4539%\n",
      "Epoch: 88 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0107, Test Acc: 53.4539%\n",
      "Epoch: 89 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0357, Test Acc: 53.6184%\n",
      "Epoch: 90 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0607, Test Acc: 53.6184%\n",
      "Epoch: 91 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0857, Test Acc: 53.6184%\n",
      "Epoch: 92 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1109, Test Acc: 53.6184%\n",
      "Epoch: 93 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1356, Test Acc: 53.7829%\n",
      "Epoch: 94 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1608, Test Acc: 53.9474%\n",
      "Epoch: 95 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1859, Test Acc: 53.9474%\n",
      "Epoch: 96 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2108, Test Acc: 53.9474%\n",
      "Epoch: 97 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2357, Test Acc: 54.1118%\n",
      "Epoch: 98 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2606, Test Acc: 53.9474%\n",
      "Epoch: 99 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2852, Test Acc: 53.9474%\n"
     ]
    }
   ],
   "source": [
    "train_glove(glove_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f834b-104f-407f-bab7-834b3ec196b7",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e03008-27e4-400b-9d69-78f00ecf8092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_data = WiCDataset(train_path, \"bert\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "valid_data = WiCDataset(valid_path, \"bert\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "test_data = WiCDataset(test_path, \"bert\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c56708-4b51-4d08-81a2-e8f561833de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc839d9-0f83-4a7a-8b94-89d311115fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = DNN(input_size = 52224, hidden_size=64, num_classes=1).to(device)\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(bert_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54ecd8-5540-45aa-8fee-9f497dfa8a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6675, Train Acc: 62.8328% | Test loss: 0.7042, Test Acc: 60.6908%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4787, Train Acc: 76.9601% | Test loss: 0.7606, Test Acc: 60.3618%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3525, Train Acc: 85.5584% | Test loss: 0.8406, Test Acc: 59.2105%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2663, Train Acc: 89.3861% | Test loss: 0.9444, Test Acc: 58.7171%\n",
      "Epoch: 4 \n",
      " ==========\n"
     ]
    }
   ],
   "source": [
    "train(bert_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3da7ca-fce0-4da7-badb-b0419183f02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(), 'bert_model_wic_small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd0175-5a5b-470c-90f3-2b794f8f0f32",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778814c5-c481-4ced-992a-66e36c98b5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = WiCDataset(train_path, \"gpt\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "valid_data = WiCDataset(valid_path, \"gpt\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "test_data = WiCDataset(test_path, \"gpt\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "751de384-aad0-4c59-9670-32b919478508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_model = DNN(input_size = 49920, hidden_size=64, num_classes=1).to(device)\n",
    "#gpt_model.load_state_dict(torch.load(\"gpt_model_wic_1.pth\"))\n",
    "lr = 0.001\n",
    "num_epochs = 40\n",
    "optimizer = torch.optim.Adam(gpt_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5a499f-8654-44ed-9edf-cbc64060bf08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.7276, Train Acc: 49.8706% | Test loss: 0.6927, Test Acc: 50.8224%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6832, Train Acc: 53.2729% | Test loss: 0.6934, Test Acc: 49.1776%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6932, Train Acc: 49.3343% | Test loss: 0.6934, Test Acc: 49.0132%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6839, Train Acc: 56.3609% | Test loss: 0.6922, Test Acc: 50.9868%\n",
      "Epoch: 4 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6689, Train Acc: 60.1516% | Test loss: 0.6920, Test Acc: 51.4803%\n",
      "Epoch: 5 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6414, Train Acc: 64.2567% | Test loss: 0.6945, Test Acc: 52.1382%\n",
      "Epoch: 6 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6030, Train Acc: 67.9734% | Test loss: 0.7034, Test Acc: 54.1118%\n",
      "Epoch: 7 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5688, Train Acc: 71.5052% | Test loss: 0.7139, Test Acc: 54.6053%\n",
      "Epoch: 8 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5610, Train Acc: 70.7655% | Test loss: 0.7123, Test Acc: 52.4671%\n",
      "Epoch: 9 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5335, Train Acc: 73.2433% | Test loss: 0.7332, Test Acc: 53.6184%\n",
      "Epoch: 10 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5280, Train Acc: 73.9090% | Test loss: 0.7277, Test Acc: 53.1250%\n",
      "Epoch: 11 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5226, Train Acc: 73.4652% | Test loss: 0.7399, Test Acc: 52.1382%\n",
      "Epoch: 12 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5472, Train Acc: 70.9320% | Test loss: 0.7432, Test Acc: 49.1776%\n",
      "Epoch: 13 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5565, Train Acc: 69.8040% | Test loss: 0.7561, Test Acc: 54.1118%\n",
      "Epoch: 14 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4919, Train Acc: 75.8506% | Test loss: 0.7655, Test Acc: 53.6184%\n",
      "Epoch: 15 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4754, Train Acc: 77.1635% | Test loss: 0.7852, Test Acc: 55.4276%\n",
      "Epoch: 16 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4796, Train Acc: 76.9046% | Test loss: 0.8595, Test Acc: 55.9211%\n",
      "Epoch: 17 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5064, Train Acc: 74.4083% | Test loss: 0.8559, Test Acc: 57.7303%\n",
      "Epoch: 18 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5277, Train Acc: 72.6146% | Test loss: 0.9210, Test Acc: 57.4013%\n",
      "Epoch: 19 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5115, Train Acc: 73.9275% | Test loss: 0.8092, Test Acc: 56.5789%\n",
      "Epoch: 20 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4502, Train Acc: 78.9941% | Test loss: 0.8287, Test Acc: 56.9079%\n",
      "Epoch: 21 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4436, Train Acc: 79.6043% | Test loss: 0.8250, Test Acc: 56.0855%\n",
      "Epoch: 22 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5005, Train Acc: 74.4638% | Test loss: 0.7809, Test Acc: 55.4276%\n",
      "Epoch: 23 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4347, Train Acc: 80.1220% | Test loss: 0.7822, Test Acc: 55.2632%\n",
      "Epoch: 24 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4683, Train Acc: 77.1080% | Test loss: 0.7796, Test Acc: 53.6184%\n",
      "Epoch: 25 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5129, Train Acc: 73.2433% | Test loss: 0.7788, Test Acc: 50.0000%\n",
      "Epoch: 26 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5289, Train Acc: 72.4482% | Test loss: 0.7672, Test Acc: 53.4539%\n",
      "Epoch: 27 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4346, Train Acc: 79.9926% | Test loss: 0.7873, Test Acc: 54.2763%\n",
      "Epoch: 28 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4000, Train Acc: 82.7293% | Test loss: 0.7799, Test Acc: 53.4539%\n",
      "Epoch: 29 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3937, Train Acc: 83.2655% | Test loss: 0.7906, Test Acc: 52.9605%\n",
      "Epoch: 30 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3777, Train Acc: 83.8942% | Test loss: 0.8030, Test Acc: 52.1382%\n",
      "Epoch: 31 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3773, Train Acc: 84.0422% | Test loss: 0.8175, Test Acc: 51.8092%\n",
      "Epoch: 32 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3531, Train Acc: 85.6509% | Test loss: 0.8283, Test Acc: 53.6184%\n",
      "Epoch: 33 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3746, Train Acc: 84.2086% | Test loss: 0.8844, Test Acc: 55.9211%\n",
      "Epoch: 34 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3696, Train Acc: 84.5599% | Test loss: 1.0900, Test Acc: 55.9211%\n",
      "Epoch: 35 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3721, Train Acc: 84.4120% | Test loss: 1.2723, Test Acc: 57.2368%\n",
      "Epoch: 36 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3962, Train Acc: 82.5444% | Test loss: 1.2895, Test Acc: 57.8947%\n",
      "Epoch: 37 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3633, Train Acc: 85.0222% | Test loss: 1.0876, Test Acc: 56.9079%\n",
      "Epoch: 38 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3268, Train Acc: 87.4075% | Test loss: 1.1717, Test Acc: 57.7303%\n",
      "Epoch: 39 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3160, Train Acc: 88.0362% | Test loss: 1.5739, Test Acc: 56.9079%\n"
     ]
    }
   ],
   "source": [
    "train(gpt_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1bf40f0-5155-416e-adb7-8274583ceadb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(gpt_model.state_dict(), 'gpt_model_wic_small_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e7b94-73a1-441a-b7be-77446c79832a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b501fc8-84c1-4fea-a32c-0476d5e95eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751d2ce-1dc9-400a-a779-49853301c394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd46cb7-d8b1-43a8-8a8d-613e87bc0eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c815463-6a45-4243-a4fb-b01c3e4b1343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660e64c-2729-4e59-b298-6a4ffabfa1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0205425-721b-49bc-8ea5-30dd3484eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55c854-5289-4633-8174-de82c24a2d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56ac2d-3138-4631-bb87-eb7f00ffac56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b0305-c7c0-4b83-86ea-3d29c292b180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae60a0-1e13-431a-8afe-804fed975a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3a9db-f77f-419d-95b0-a7668786e9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbf0b6-0345-4c05-bf0a-2e96221e9104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4f563-0e95-437c-a33a-484fb145aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfabf5-8623-4c57-ba9d-3c0f61c6be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0006565-394d-47cd-8671-76ddf35c7c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b390560-a637-43b7-a0e2-d615d7454fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48da91f-79af-422e-b60d-eb955e542666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0908c-2b5c-45dc-a60e-9899c8d1bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a7914-be59-4fb4-8959-2355868fb9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dcfb3-aab2-438e-bfc7-461f16861219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16add5-7815-4dd7-9205-958c5d1189a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710d66-f558-4ec8-97a8-ba7046298dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04c418-0ebb-4170-bde8-f228b26fb21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c32b-ac46-4b3c-9500-8ca06ef925a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6854c1a-0236-4018-bfea-55420d60a379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217901fa-14b9-4f71-8d51-b96441c9880b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b643562-3521-4c0f-823b-bf24f4c5dbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594bd16-c7b4-4535-8d13-eb04f9f1525c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a3dab-119e-4dd3-bfe5-390ed6a669ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab84bb6-57d0-4506-855d-c839957d1a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b662c6-7348-44b6-b24c-645536c36795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53332a-f7e5-4acd-9674-d103b4b2743a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589595c7-71a6-4105-95fa-35863038b692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cc0a9-8570-4fa3-b15e-e14d8f524efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255e536-46ef-4df3-9ba3-e726489a572b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b39863-47c9-4aea-ad69-ded3dcb3caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee083b-67c2-404a-a565-4aa86418ad4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e04e2-1227-425f-8684-e3bc21e1c436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5eb14-2438-438b-b9fb-414e342f3fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d7349-3aaf-40ab-b9bf-e0d910e94dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5e2e0-9383-41c5-8607-68928c536e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2fa89-9303-4fa7-b909-93d024d5bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f817c-6a46-40e2-9823-e853cdb996fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838b94a-69bd-46b4-a4c6-977a52f96d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa210325-24e6-4d6c-bac3-c233ad3c1b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c51cf1-6394-4735-a954-fa935fd74db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56d606-8a88-4417-a165-1760ec129518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937015e-2c4d-4226-9f39-c19aab33b949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e9415-66fb-4f6b-89ae-83ce80fe844b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e1a31-2a35-4831-8356-a5ee0a8c8d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dcfa6-5b1a-4d3e-a400-0771d04484b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344b8ad-a2aa-4bb8-8ae3-44bf3d4ae781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
