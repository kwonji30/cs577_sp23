{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f898e0f-bceb-4c4a-b395-f951e810e61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6755248a-5f9b-4d9e-a65c-92edfdd16d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623a8216-265f-4817-8320-14d28d34fad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d98a61-319d-40e5-a6f5-6b6128f8b5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) <= 65:\n",
    "            padded_seq = torch.nn.functional.pad(seq, (0, 0, 0, 65 - seq.size(0)), mode='constant', value=0)\n",
    "        else:\n",
    "            print(sequences.numel())\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9266d89-85a4-4e8b-b85a-7b01307fbcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.model = AutoModel.from_pretrained('gpt2').to(device)\n",
    "        self.data = pd.read_csv(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "        gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "        padded_outputs = pad_sequences(gpt_outputs)\n",
    "        return (padded_outputs, torch.tensor(self.data['Label'].iloc[idx], dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364afc80-f188-420b-b0aa-231c6278bf50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = customDataset(\"small_data.csv\")\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=data,\n",
    "                                              batch_size=32,\n",
    "                                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b54b3b-9fc7-40ac-8d27-8321e42e4e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_model = DNN(input_size=768*65, hidden_size=62, num_classes=1).to(device)\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "optimizer = torch.optim.Adam(gpt_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e909d6d-da6c-4cd0-a35f-b7cdd622a711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.7259482145309448, Train Acc: 58.45044378698225\n",
      "Epoch: 1 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.592014729976654, Train Acc: 67.89940828402366\n",
      "Epoch: 2 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.5279016494750977, Train Acc: 73.79807692307692\n",
      "Epoch: 3 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4704250991344452, Train Acc: 79.25295857988165\n",
      "Epoch: 4 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4236723780632019, Train Acc: 83.52440828402366\n",
      "Epoch: 5 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3975466191768646, Train Acc: 86.5939349112426\n",
      "Epoch: 6 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.443880558013916, Train Acc: 82.91420118343196\n",
      "Epoch: 7 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.47273707389831543, Train Acc: 81.63831360946746\n",
      "Epoch: 8 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.45010557770729065, Train Acc: 82.50739644970415\n",
      "Epoch: 9 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.45523035526275635, Train Acc: 82.30399408284023\n",
      "Epoch: 10 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4355543255805969, Train Acc: 84.41198224852072\n",
      "Epoch: 11 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.4308825135231018, Train Acc: 84.13461538461539\n",
      "Epoch: 12 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.41202419996261597, Train Acc: 85.72485207100591\n",
      "Epoch: 13 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3858277499675751, Train Acc: 87.70340236686391\n",
      "Epoch: 14 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3752410113811493, Train Acc: 88.48002958579882\n",
      "Epoch: 15 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.37266382575035095, Train Acc: 88.68343195266272\n",
      "Epoch: 16 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.36588940024375916, Train Acc: 89.70044378698225\n",
      "Epoch: 17 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.35201457142829895, Train Acc: 91.01331360946746\n",
      "Epoch: 18 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.35856813192367554, Train Acc: 90.77292899408285\n",
      "Epoch: 19 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.38368749618530273, Train Acc: 88.03624260355029\n",
      "Epoch: 20 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.38600027561187744, Train Acc: 86.98224852071006\n",
      "Epoch: 21 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3802545666694641, Train Acc: 86.46449704142012\n",
      "Epoch: 22 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.357336163520813, Train Acc: 90.12573964497041\n",
      "Epoch: 23 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3569728434085846, Train Acc: 89.75591715976331\n",
      "Epoch: 24 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3542768657207489, Train Acc: 90.12573964497041\n",
      "Epoch: 25 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.36003008484840393, Train Acc: 89.44156804733728\n",
      "Epoch: 26 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3581198453903198, Train Acc: 89.07174556213018\n",
      "Epoch: 27 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.33870694041252136, Train Acc: 90.82840236686391\n",
      "Epoch: 28 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3367391526699066, Train Acc: 90.9948224852071\n",
      "Epoch: 29 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.32947200536727905, Train Acc: 91.78994082840237\n",
      "Epoch: 30 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.32437944412231445, Train Acc: 92.65902366863905\n",
      "Epoch: 31 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.323039710521698, Train Acc: 92.49260355029585\n",
      "Epoch: 32 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3229126036167145, Train Acc: 92.9733727810651\n",
      "Epoch: 33 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3166780173778534, Train Acc: 93.06582840236686\n",
      "Epoch: 34 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3201393187046051, Train Acc: 93.04733727810651\n",
      "Epoch: 35 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.34700921177864075, Train Acc: 89.07174556213018\n",
      "Epoch: 36 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3719682991504669, Train Acc: 85.74334319526628\n",
      "Epoch: 37 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.39263084530830383, Train Acc: 81.89718934911242\n",
      "Epoch: 38 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3501583933830261, Train Acc: 87.77736686390533\n",
      "Epoch: 39 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.35929855704307556, Train Acc: 87.8698224852071\n",
      "Epoch: 40 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.36939388513565063, Train Acc: 87.42603550295858\n",
      "Epoch: 41 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.40970292687416077, Train Acc: 74.18639053254438\n",
      "Epoch: 42 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.327155202627182, Train Acc: 91.12426035502959\n",
      "Epoch: 43 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3205123543739319, Train Acc: 91.9008875739645\n",
      "Epoch: 44 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3141724765300751, Train Acc: 92.95488165680473\n",
      "Epoch: 45 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.31625282764434814, Train Acc: 93.15828402366864\n",
      "Epoch: 46 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3398362100124359, Train Acc: 89.5155325443787\n",
      "Epoch: 47 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3218085765838623, Train Acc: 90.88387573964496\n",
      "Epoch: 48 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3044184148311615, Train Acc: 93.56508875739645\n",
      "Epoch: 49 \n",
      " =====================\n",
      "\n",
      "Train Loss: 0.3007984757423401, Train Acc: 94.10133136094674\n",
      "Epoch: 50 \n",
      " =====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Perform gradient descent\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch} \\n =====================\")\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "        gpt_model.train()\n",
    "        # Forward Pass\n",
    "        logits = gpt_model(flattened_inputs).squeeze()\n",
    "        rounded_labels = torch.round(labels)\n",
    "        pred = torch.round(torch.sigmoid(logits))\n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(rounded_labels, pred)\n",
    "        # Zero the graident\n",
    "        optimizer.zero_grad()\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "        # Perform gradient descent\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    print(f\"\\nTrain Loss: {train_loss}, Train Acc: {train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64c7bd7-c1c9-4ede-8650-ad4f8fddbf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(gpt_model.state_dict(), 'student_model_wic_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53217b76-f2f5-4e18-bd31-ffec1747a590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df_data = pd.read_csv(path+\"data.txt\",\n",
    "                              delimiter='\\t',\n",
    "                              names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "        df_label = pd.read_csv(path+'gold.txt',\n",
    "                               delimiter='\\t',\n",
    "                               names=['label'])\n",
    "        self.data = pd.concat([df_data, df_label], axis=1)\n",
    "        self.data['Joined'] = self.data['Context1'] + \" \" + self.data['Context2']\n",
    "        self.data['label'] = self.data['label'].map(lambda x: 0 if x == 'F' else 1)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.model = AutoModel.from_pretrained('gpt2').to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "        gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "        padded_outputs = pad_sequences(gpt_outputs)\n",
    "        return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \n",
    "        \n",
    "test_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\test\\test.\"\n",
    "test_data = testDataset(test_path)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                             batch_size=32,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f4eab-1cf1-4c2e-9feb-2139096fe193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DNN(input_size=768*65, hidden_size=52, num_classes=1).to(device)\n",
    "# model.load_state_dict(torch.load('student_model_wic_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea17c32d-c735-45c2-87e8-6abacf559c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_list, label_list = [], []\n",
    "gpt_model.eval()\n",
    "with torch.inference_mode():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "        test_logits = gpt_model(flattened_inputs)\n",
    "        pred = torch.round(torch.sigmoid(test_logits))\n",
    "        pred_list.append(pred.cpu().numpy())\n",
    "        label_list.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd2a6d01-d1f6-468e-8144-901c3d78f2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_array = np.concatenate(pred_list)\n",
    "label_array = np.concatenate(label_list)\n",
    "pred_array  = pred_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abfc437-9c4a-42be-90e0-1f5b80d02fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_array == label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70fb75-4523-43f3-b906-bf6e9a77833a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
