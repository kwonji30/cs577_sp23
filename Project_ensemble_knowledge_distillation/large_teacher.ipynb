{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e801bfd-1f03-4015-b0ad-14efb195111c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joowa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "glove = api.load('glove-wiki-gigaword-50')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bafe91-b844-4dab-9abe-1b4c6a1557ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, num_epochs, device, accuracy_fn):\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch} \\n ==========\")\n",
    "        ### Training\n",
    "        train_loss, train_acc = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "            model.train()\n",
    "            # Forard Pass\n",
    "            logits = model(flattened_inputs).squeeze()\n",
    "            pred = torch.round(torch.sigmoid(logits))\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            train_loss += loss\n",
    "            train_acc += accuracy_fn(labels, pred)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Perform gradient descent\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        ### Testing\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inptus, labels = inputs.to(device), labels.to(device)\n",
    "                flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "                # Forward pass\n",
    "                test_logits = model(flattened_inputs).squeeze()\n",
    "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "                # Calculate the loss and accuracy\n",
    "                test_loss += criterion(test_logits, labels)\n",
    "                test_acc += accuracy_fn(labels, test_pred)\n",
    "            test_loss /= len(valid_loader)\n",
    "            test_acc /= len(valid_loader)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}% | Test loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9018e209-2ec3-4f5f-a8fb-3ca5668e0c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_glove(model, optimizer, criterion, train_loader, valid_loader, num_epochs, device, accuracy_fn):\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch} \\n ==========\")\n",
    "        ### Training\n",
    "        train_loss, train_acc = 0, 0\n",
    "        for inputs, labels, seq_lengths in train_loader:\n",
    "            inputs, labels, seq_lengths = inputs.to(device), labels.to(device), seq_lengths.to(device)\n",
    "            model.train()\n",
    "            # Forard Pass\n",
    "            logits = model(inputs, seq_lengths).squeeze()\n",
    "            pred = torch.round(torch.sigmoid(logits))\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            train_loss += loss\n",
    "            train_acc += accuracy_fn(labels, pred)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Perform gradient descent\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        ### Testing\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels, seq_lengths in valid_loader:\n",
    "                inptus, labels, seq_lengths = inputs.to(device), labels.to(device), seq_lengths.to(device)\n",
    "                # Forward pass\n",
    "                test_logits = model(inputs, seq_lengths).squeeze()\n",
    "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "                # Calculate the loss and accuracy\n",
    "                test_loss += criterion(test_logits, labels)\n",
    "                test_acc += accuracy_fn(labels, test_pred)\n",
    "            test_loss /= len(valid_loader)\n",
    "            test_acc /= len(valid_loader)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f\"\\nTrain loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}% | Test loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6795cdd2-18ec-453e-805c-ff1e10f8ea68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c755a4a7-354f-4be2-8e22-cef128bfd518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    # pad the inputs with zeros to make them the same length\n",
    "    inputs_padded = rnn_utils.pad_sequence(inputs, batch_first=True)\n",
    "    # get the sequence lenghts of the inputs\n",
    "    seq_length = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    \n",
    "    # sort the inputs and labels by the sequence lengths\n",
    "    seq_length, sort_idx = seq_length.sort(descending=True)\n",
    "    inputs_padded = inputs_padded[sort_idx].to(device)\n",
    "    labels_sorted = torch.tensor(labels, dtype=torch.float32)[sort_idx].to(device)\n",
    "\n",
    "    return inputs_padded, labels_sorted, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84abde47-1ead-4c10-beaa-45bab3ff7996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Divides a list into sublists with an equal amount of items in each sublist.\"\"\"\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fc0d9f-00f5-4859-b4cd-06043110ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_maxLength(sentence_list, tokenizer):\n",
    "    length_list = []\n",
    "    sentence_lists = chunk_list(sentence_list, 200)\n",
    "    for block in sentence_lists:\n",
    "        torch.cuda.empty_cache()\n",
    "        token = tokenizer(block,\n",
    "                          padding=True,\n",
    "                          return_tensors='pt')\n",
    "        length_list.append(token['input_ids'].shape[1])\n",
    "    return max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af147be-789d-43a1-ab9b-4f08e29982c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) <= 65:\n",
    "            padded_seq = torch.nn.functional.pad(seq, (0, 0, 0, 65 - seq.size(0)), mode='constant', value=0)\n",
    "        else:\n",
    "            print(sequences.numel())\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89aa6956-927a-4604-8566-825c17f5c65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "class WiCDataset(Dataset):\n",
    "    def __init__(self, path, mode):\n",
    "        self.mode = mode\n",
    "        if mode == \"gpt\":\n",
    "            self.mode = 'gpt2'\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "        elif mode == \"bert\":\n",
    "            self.mode = 'bert-base-uncased'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.mode) \n",
    "            self.model = AutoModel.from_pretrained(self.mode).to(device)\n",
    "\n",
    "        df_data = pd.read_csv(path+\"data.txt\",\n",
    "                              delimiter='\\t',\n",
    "                              names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "        df_label = pd.read_csv(path+'gold.txt',\n",
    "                               delimiter='\\t',\n",
    "                               names=['label'])\n",
    "        self.data = pd.concat([df_data, df_label], axis=1)\n",
    "        self.data['Joined'] = self.data['Context1'] + \" \" + self.data['Context2']\n",
    "        self.data['label'] = self.data['label'].map(lambda x: 0 if x == 'F' else 1)\n",
    "        #self.maxLength = find_maxLength(self.data['Joined'].tolist(), self.tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'gpt2':\n",
    "           # self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "           # gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "            padded_outputs = pad_sequences(gpt_outputs)\n",
    "           # with torch.inference_mode():\n",
    "           #     gpt_outputs = self.model(**gpt_token)\n",
    "            return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \n",
    "            \n",
    "            # sentence_lists = chunk_list(self.data['Joined'].tolist(), 200)\n",
    "            # tensor_list = []\n",
    "            # for block in sentence_lists:\n",
    "            #     torch.cuda.empty_cache()\n",
    "            #     gpt_token = self.tokenizer(block, padding='max_length', return_tensors='pt', max_length=65).to(device)\n",
    "            #     with torch.inference_mode():\n",
    "            #         gpt_outputs = self.model(**gpt_token)\n",
    "            #     tensor_list.append(gpt_outputs[0])\n",
    "            # gpt_tensor = torch.cat(tensor_list, dim = 0)\n",
    "            # return (gpt_tensor[idx].cpu(), torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long))\n",
    "        \n",
    "        \n",
    "        elif self.mode == 'bert-base-uncased':\n",
    "            bert_token = self.tokenizer(self.data['Joined'].iloc[idx], padding='max_length', return_tensors='pt', max_length=68).to(device)        \n",
    "            with torch.inference_mode():\n",
    "                bert_outputs = self.model(**bert_token)\n",
    "            return (bert_outputs[0], torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32))\n",
    "        \n",
    "            \n",
    "        elif self.mode == 'glove':\n",
    "            row = self.data.iloc[idx]\n",
    "            words = word_tokenize(row.Joined.lower())\n",
    "\n",
    "            indices = [glove.get_index(w) for w in words if glove.has_index_for(w)]\n",
    "            indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "            return indices_tensor, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "train_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\train\\train.\"\n",
    "valid_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\dev\\dev.\"\n",
    "test_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\test\\test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf84704-ab3a-45f2-b1cf-d21388c3f7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = WiCDataset(train_path, \"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe0f7cd-db5b-4688-ad45-cdf44834a9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(temp,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "952a21ab-2746-4f37-a4cd-5a9610002f0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n",
      "torch.Size([32, 1, 65, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[33], line 35\u001b[0m, in \u001b[0;36mWiCDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m             gpt_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJoined\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idx], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 35\u001b[0m                 gpt_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgpt_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (gpt_outputs[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)) \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#            # gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#             gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#             gpt_outputs = self.model(gpt_token['input_ids'])[0]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#            #     gpt_outputs = self.model(**gpt_token)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#             return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    387\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 389\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    398\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:185\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    182\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[1;32m--> 185\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Layer-wise attention scaling\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_by_inverse_layer_idx:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for inputs, labels in data_loader:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613e90c-7e12-4f20-bec4-9ea7a6f9d959",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc1cefb-521e-4162-9da5-8c68d0915a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = WiCDataset(train_path, \"glove\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "valid_data = WiCDataset(valid_path, \"glove\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "test_data = WiCDataset(test_path, \"glove\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd38dfc2-b0bc-49e4-a03d-b010c12193a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.FloatTensor(glove.vectors))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers,\n",
    "                          bidirectional = True,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, seq, seq_length):\n",
    "        inputs_embedded = self.emb(seq)\n",
    "        seq_length = seq_length.cpu()\n",
    "        packed_input = rnn_utils.pack_padded_sequence(inputs_embedded, seq_length, batch_first=True)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), seq_length - 1, :self.hidden_dim]\n",
    "        out_reverse = output[:, 0, self.hidden_dim:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        output = self.fc(out_reduced)\n",
    "        return output\n",
    "\n",
    "glove_model = LSTM(50, 128, 1, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908920a-3996-4c5b-bd6e-77d6d2a799e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(glove_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80434739-dfcb-4d9b-ae59-4dac613af8a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0549, Train Acc: 98.1509% | Test loss: 2.3807, Test Acc: 54.1118%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0406, Train Acc: 98.6317% | Test loss: 2.6156, Test Acc: 52.1382%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0468, Train Acc: 98.4837% | Test loss: 2.4443, Test Acc: 51.8092%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0369, Train Acc: 98.8166% | Test loss: 2.6459, Test Acc: 53.7829%\n",
      "Epoch: 4 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0337, Train Acc: 98.8905% | Test loss: 2.7209, Test Acc: 54.7697%\n",
      "Epoch: 5 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0333, Train Acc: 98.9460% | Test loss: 2.5958, Test Acc: 53.6184%\n",
      "Epoch: 6 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0309, Train Acc: 99.1309% | Test loss: 2.9188, Test Acc: 51.8092%\n",
      "Epoch: 7 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0227, Train Acc: 99.3528% | Test loss: 2.8814, Test Acc: 53.4539%\n",
      "Epoch: 8 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0192, Train Acc: 99.4822% | Test loss: 2.8821, Test Acc: 54.1118%\n",
      "Epoch: 9 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0101, Train Acc: 99.7411% | Test loss: 3.0779, Test Acc: 53.6184%\n",
      "Epoch: 10 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0100, Train Acc: 99.7411% | Test loss: 3.1492, Test Acc: 55.0987%\n",
      "Epoch: 11 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0086, Train Acc: 99.8336% | Test loss: 3.2914, Test Acc: 55.5921%\n",
      "Epoch: 12 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0158, Train Acc: 99.6487% | Test loss: 3.1281, Test Acc: 51.6447%\n",
      "Epoch: 13 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0576, Train Acc: 97.8920% | Test loss: 2.6821, Test Acc: 54.6053%\n",
      "Epoch: 14 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0398, Train Acc: 98.6132% | Test loss: 2.5901, Test Acc: 54.1118%\n",
      "Epoch: 15 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0178, Train Acc: 99.5562% | Test loss: 2.7512, Test Acc: 53.4539%\n",
      "Epoch: 16 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0077, Train Acc: 99.7966% | Test loss: 2.9299, Test Acc: 54.9342%\n",
      "Epoch: 17 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0063, Train Acc: 99.9075% | Test loss: 2.9570, Test Acc: 54.9342%\n",
      "Epoch: 18 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0057, Train Acc: 99.8336% | Test loss: 3.0853, Test Acc: 55.2632%\n",
      "Epoch: 19 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0060, Train Acc: 99.8521% | Test loss: 3.2133, Test Acc: 54.9342%\n",
      "Epoch: 20 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0264, Train Acc: 99.2604% | Test loss: 2.9821, Test Acc: 54.1118%\n",
      "Epoch: 21 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0317, Train Acc: 98.9830% | Test loss: 2.9324, Test Acc: 54.2763%\n",
      "Epoch: 22 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0196, Train Acc: 99.3898% | Test loss: 2.9444, Test Acc: 54.6053%\n",
      "Epoch: 23 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0078, Train Acc: 99.7966% | Test loss: 3.0229, Test Acc: 55.7566%\n",
      "Epoch: 24 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0053, Train Acc: 99.9445% | Test loss: 3.0416, Test Acc: 55.4276%\n",
      "Epoch: 25 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0064, Train Acc: 99.9075% | Test loss: 3.1762, Test Acc: 53.7829%\n",
      "Epoch: 26 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0012, Train Acc: 99.9815% | Test loss: 3.3046, Test Acc: 53.9474%\n",
      "Epoch: 27 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0004, Train Acc: 100.0000% | Test loss: 3.3653, Test Acc: 54.1118%\n",
      "Epoch: 28 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0003, Train Acc: 100.0000% | Test loss: 3.4134, Test Acc: 54.2763%\n",
      "Epoch: 29 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0002, Train Acc: 100.0000% | Test loss: 3.4558, Test Acc: 54.2763%\n",
      "Epoch: 30 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0002, Train Acc: 100.0000% | Test loss: 3.4945, Test Acc: 54.4408%\n",
      "Epoch: 31 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5307, Test Acc: 54.9342%\n",
      "Epoch: 32 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5647, Test Acc: 54.9342%\n",
      "Epoch: 33 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.5973, Test Acc: 54.7697%\n",
      "Epoch: 34 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6287, Test Acc: 54.9342%\n",
      "Epoch: 35 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6588, Test Acc: 54.7697%\n",
      "Epoch: 36 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.6883, Test Acc: 54.6053%\n",
      "Epoch: 37 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7171, Test Acc: 54.4408%\n",
      "Epoch: 38 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7453, Test Acc: 54.4408%\n",
      "Epoch: 39 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 3.7730, Test Acc: 54.4408%\n",
      "Epoch: 40 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8002, Test Acc: 54.4408%\n",
      "Epoch: 41 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8273, Test Acc: 54.4408%\n",
      "Epoch: 42 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8540, Test Acc: 54.4408%\n",
      "Epoch: 43 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.8802, Test Acc: 54.1118%\n",
      "Epoch: 44 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9062, Test Acc: 54.1118%\n",
      "Epoch: 45 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9320, Test Acc: 54.2763%\n",
      "Epoch: 46 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9577, Test Acc: 54.2763%\n",
      "Epoch: 47 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 3.9833, Test Acc: 54.1118%\n",
      "Epoch: 48 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0087, Test Acc: 54.2763%\n",
      "Epoch: 49 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0340, Test Acc: 54.2763%\n",
      "Epoch: 50 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0592, Test Acc: 54.2763%\n",
      "Epoch: 51 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.0843, Test Acc: 54.2763%\n",
      "Epoch: 52 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1094, Test Acc: 54.2763%\n",
      "Epoch: 53 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1345, Test Acc: 54.2763%\n",
      "Epoch: 54 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1594, Test Acc: 54.1118%\n",
      "Epoch: 55 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.1844, Test Acc: 54.2763%\n",
      "Epoch: 56 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2090, Test Acc: 54.2763%\n",
      "Epoch: 57 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2341, Test Acc: 54.2763%\n",
      "Epoch: 58 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2589, Test Acc: 54.2763%\n",
      "Epoch: 59 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.2839, Test Acc: 54.2763%\n",
      "Epoch: 60 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3087, Test Acc: 54.2763%\n",
      "Epoch: 61 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3336, Test Acc: 54.1118%\n",
      "Epoch: 62 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3585, Test Acc: 54.1118%\n",
      "Epoch: 63 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.3833, Test Acc: 54.1118%\n",
      "Epoch: 64 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4084, Test Acc: 54.1118%\n",
      "Epoch: 65 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4333, Test Acc: 54.1118%\n",
      "Epoch: 66 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4583, Test Acc: 53.9474%\n",
      "Epoch: 67 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.4832, Test Acc: 53.9474%\n",
      "Epoch: 68 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5082, Test Acc: 53.9474%\n",
      "Epoch: 69 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5331, Test Acc: 53.9474%\n",
      "Epoch: 70 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5582, Test Acc: 53.9474%\n",
      "Epoch: 71 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.5833, Test Acc: 53.9474%\n",
      "Epoch: 72 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6084, Test Acc: 54.1118%\n",
      "Epoch: 73 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6334, Test Acc: 54.1118%\n",
      "Epoch: 74 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6586, Test Acc: 53.9474%\n",
      "Epoch: 75 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.6838, Test Acc: 53.9474%\n",
      "Epoch: 76 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7089, Test Acc: 53.9474%\n",
      "Epoch: 77 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7339, Test Acc: 53.9474%\n",
      "Epoch: 78 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7592, Test Acc: 53.7829%\n",
      "Epoch: 79 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7844, Test Acc: 53.6184%\n",
      "Epoch: 80 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8095, Test Acc: 53.6184%\n",
      "Epoch: 81 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8348, Test Acc: 53.6184%\n",
      "Epoch: 82 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8600, Test Acc: 53.6184%\n",
      "Epoch: 83 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8850, Test Acc: 53.6184%\n",
      "Epoch: 84 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9102, Test Acc: 53.6184%\n",
      "Epoch: 85 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9354, Test Acc: 53.6184%\n",
      "Epoch: 86 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9603, Test Acc: 53.4539%\n",
      "Epoch: 87 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9856, Test Acc: 53.4539%\n",
      "Epoch: 88 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0107, Test Acc: 53.4539%\n",
      "Epoch: 89 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0357, Test Acc: 53.6184%\n",
      "Epoch: 90 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0607, Test Acc: 53.6184%\n",
      "Epoch: 91 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0857, Test Acc: 53.6184%\n",
      "Epoch: 92 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1109, Test Acc: 53.6184%\n",
      "Epoch: 93 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1356, Test Acc: 53.7829%\n",
      "Epoch: 94 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1608, Test Acc: 53.9474%\n",
      "Epoch: 95 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1859, Test Acc: 53.9474%\n",
      "Epoch: 96 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2108, Test Acc: 53.9474%\n",
      "Epoch: 97 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2357, Test Acc: 54.1118%\n",
      "Epoch: 98 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2606, Test Acc: 53.9474%\n",
      "Epoch: 99 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2852, Test Acc: 53.9474%\n"
     ]
    }
   ],
   "source": [
    "train_glove(glove_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f834b-104f-407f-bab7-834b3ec196b7",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e03008-27e4-400b-9d69-78f00ecf8092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_data = WiCDataset(train_path, \"bert\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "valid_data = WiCDataset(valid_path, \"bert\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "test_data = WiCDataset(test_path, \"bert\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c56708-4b51-4d08-81a2-e8f561833de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc839d9-0f83-4a7a-8b94-89d311115fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = DNN(input_size = 52224, hidden_size=512, num_classes=1).to(device)\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(bert_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f54ecd8-5540-45aa-8fee-9f497dfa8a59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6852, Train Acc: 60.9467% | Test loss: 0.6939, Test Acc: 58.0592%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5206, Train Acc: 73.7241% | Test loss: 0.7360, Test Acc: 61.0197%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3834, Train Acc: 83.0806% | Test loss: 0.9730, Test Acc: 58.3882%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3052, Train Acc: 87.1487% | Test loss: 1.3068, Test Acc: 54.7697%\n",
      "Epoch: 4 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2593, Train Acc: 88.9978% | Test loss: 1.4106, Test Acc: 58.2237%\n",
      "Epoch: 5 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1941, Train Acc: 91.8824% | Test loss: 1.6193, Test Acc: 60.0329%\n",
      "Epoch: 6 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1868, Train Acc: 93.0288% | Test loss: 1.5425, Test Acc: 60.0329%\n",
      "Epoch: 7 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1654, Train Acc: 93.4541% | Test loss: 1.8533, Test Acc: 58.2237%\n",
      "Epoch: 8 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1313, Train Acc: 94.4342% | Test loss: 2.1485, Test Acc: 57.7303%\n",
      "Epoch: 9 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1218, Train Acc: 95.3587% | Test loss: 1.9593, Test Acc: 56.0855%\n",
      "Epoch: 10 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1234, Train Acc: 95.2478% | Test loss: 3.4564, Test Acc: 53.4539%\n",
      "Epoch: 11 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1370, Train Acc: 94.4342% | Test loss: 1.9926, Test Acc: 59.2105%\n",
      "Epoch: 12 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0998, Train Acc: 95.9320% | Test loss: 2.2168, Test Acc: 59.8684%\n",
      "Epoch: 13 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0626, Train Acc: 97.2448% | Test loss: 2.4618, Test Acc: 61.3487%\n",
      "Epoch: 14 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0484, Train Acc: 98.2433% | Test loss: 2.8860, Test Acc: 56.5789%\n",
      "Epoch: 15 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0485, Train Acc: 98.3173% | Test loss: 2.7079, Test Acc: 61.3487%\n",
      "Epoch: 16 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0435, Train Acc: 98.6132% | Test loss: 2.3492, Test Acc: 61.1842%\n",
      "Epoch: 17 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0468, Train Acc: 98.4652% | Test loss: 2.5943, Test Acc: 61.5132%\n",
      "Epoch: 18 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0708, Train Acc: 97.5407% | Test loss: 2.0888, Test Acc: 60.1974%\n",
      "Epoch: 19 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0734, Train Acc: 97.5222% | Test loss: 2.3786, Test Acc: 59.5395%\n",
      "Epoch: 20 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0498, Train Acc: 98.1324% | Test loss: 2.6681, Test Acc: 58.5526%\n",
      "Epoch: 21 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0440, Train Acc: 98.4837% | Test loss: 2.3649, Test Acc: 60.1974%\n",
      "Epoch: 22 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0322, Train Acc: 98.6686% | Test loss: 2.7806, Test Acc: 60.3618%\n",
      "Epoch: 23 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0347, Train Acc: 98.6686% | Test loss: 2.8607, Test Acc: 59.0461%\n",
      "Epoch: 24 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0244, Train Acc: 99.1494% | Test loss: 2.7197, Test Acc: 59.2105%\n",
      "Epoch: 25 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0164, Train Acc: 99.4268% | Test loss: 3.3005, Test Acc: 59.8684%\n",
      "Epoch: 26 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0205, Train Acc: 99.2234% | Test loss: 3.4280, Test Acc: 59.3750%\n",
      "Epoch: 27 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0155, Train Acc: 99.5562% | Test loss: 3.1446, Test Acc: 61.8421%\n",
      "Epoch: 28 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0196, Train Acc: 99.3343% | Test loss: 3.3672, Test Acc: 59.3750%\n",
      "Epoch: 29 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0193, Train Acc: 99.3898% | Test loss: 3.6259, Test Acc: 60.3618%\n",
      "Epoch: 30 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0142, Train Acc: 99.5377% | Test loss: 3.6474, Test Acc: 59.5395%\n",
      "Epoch: 31 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0174, Train Acc: 99.3898% | Test loss: 3.4325, Test Acc: 59.3750%\n",
      "Epoch: 32 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0150, Train Acc: 99.5007% | Test loss: 3.9083, Test Acc: 58.0592%\n",
      "Epoch: 33 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0119, Train Acc: 99.6302% | Test loss: 3.4038, Test Acc: 60.5263%\n",
      "Epoch: 34 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0156, Train Acc: 99.4638% | Test loss: 3.6897, Test Acc: 59.7039%\n",
      "Epoch: 35 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0111, Train Acc: 99.6302% | Test loss: 3.9789, Test Acc: 61.3487%\n",
      "Epoch: 36 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0165, Train Acc: 99.4822% | Test loss: 4.0756, Test Acc: 58.7171%\n",
      "Epoch: 37 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0233, Train Acc: 99.3528% | Test loss: 3.5922, Test Acc: 59.3750%\n",
      "Epoch: 38 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0106, Train Acc: 99.7041% | Test loss: 3.9324, Test Acc: 60.5263%\n",
      "Epoch: 39 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0150, Train Acc: 99.6487% | Test loss: 3.4327, Test Acc: 62.3355%\n",
      "Epoch: 40 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0151, Train Acc: 99.5377% | Test loss: 3.3929, Test Acc: 59.7039%\n",
      "Epoch: 41 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0129, Train Acc: 99.5377% | Test loss: 3.6041, Test Acc: 60.8553%\n",
      "Epoch: 42 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0062, Train Acc: 99.7966% | Test loss: 3.8725, Test Acc: 61.0197%\n",
      "Epoch: 43 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0144, Train Acc: 99.6672% | Test loss: 3.6806, Test Acc: 59.7039%\n",
      "Epoch: 44 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0196, Train Acc: 99.5192% | Test loss: 3.6423, Test Acc: 61.6776%\n",
      "Epoch: 45 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0150, Train Acc: 99.6487% | Test loss: 3.2576, Test Acc: 60.8553%\n",
      "Epoch: 46 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0109, Train Acc: 99.6302% | Test loss: 3.7986, Test Acc: 60.6908%\n",
      "Epoch: 47 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0245, Train Acc: 99.5562% | Test loss: 4.2079, Test Acc: 61.5132%\n",
      "Epoch: 48 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0155, Train Acc: 99.5377% | Test loss: 3.2273, Test Acc: 60.1974%\n",
      "Epoch: 49 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0150, Train Acc: 99.5562% | Test loss: 3.3646, Test Acc: 61.6776%\n",
      "Epoch: 50 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0098, Train Acc: 99.8151% | Test loss: 3.2425, Test Acc: 62.6645%\n",
      "Epoch: 51 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0052, Train Acc: 99.8521% | Test loss: 3.7536, Test Acc: 61.8421%\n",
      "Epoch: 52 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0106, Train Acc: 99.9075% | Test loss: 3.5347, Test Acc: 61.0197%\n",
      "Epoch: 53 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0084, Train Acc: 99.6672% | Test loss: 3.2375, Test Acc: 61.6776%\n",
      "Epoch: 54 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0182, Train Acc: 99.4453% | Test loss: 4.0401, Test Acc: 60.0329%\n",
      "Epoch: 55 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0115, Train Acc: 99.6117% | Test loss: 3.5511, Test Acc: 60.5263%\n",
      "Epoch: 56 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0148, Train Acc: 99.5192% | Test loss: 3.4273, Test Acc: 60.3618%\n",
      "Epoch: 57 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0194, Train Acc: 99.4638% | Test loss: 2.8767, Test Acc: 58.7171%\n",
      "Epoch: 58 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0121, Train Acc: 99.6487% | Test loss: 3.9026, Test Acc: 58.7171%\n",
      "Epoch: 59 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0181, Train Acc: 99.4453% | Test loss: 3.0795, Test Acc: 58.8816%\n",
      "Epoch: 60 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0101, Train Acc: 99.6857% | Test loss: 3.4015, Test Acc: 58.2237%\n",
      "Epoch: 61 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0087, Train Acc: 99.7226% | Test loss: 3.3773, Test Acc: 56.4145%\n",
      "Epoch: 62 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0036, Train Acc: 99.8336% | Test loss: 3.6867, Test Acc: 60.1974%\n",
      "Epoch: 63 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0115, Train Acc: 99.5932% | Test loss: 3.3234, Test Acc: 60.1974%\n",
      "Epoch: 64 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0029, Train Acc: 99.9075% | Test loss: 3.7819, Test Acc: 60.3618%\n",
      "Epoch: 65 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0008, Train Acc: 100.0000% | Test loss: 4.2380, Test Acc: 60.3618%\n",
      "Epoch: 66 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0026, Train Acc: 99.9630% | Test loss: 4.6605, Test Acc: 58.5526%\n",
      "Epoch: 67 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0112, Train Acc: 99.6302% | Test loss: 3.6508, Test Acc: 59.2105%\n",
      "Epoch: 68 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0254, Train Acc: 99.3713% | Test loss: 2.8290, Test Acc: 61.3487%\n",
      "Epoch: 69 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0221, Train Acc: 99.3343% | Test loss: 3.1369, Test Acc: 60.5263%\n",
      "Epoch: 70 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0142, Train Acc: 99.6117% | Test loss: 3.3671, Test Acc: 60.8553%\n",
      "Epoch: 71 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0125, Train Acc: 99.5747% | Test loss: 4.0099, Test Acc: 59.0461%\n",
      "Epoch: 72 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0273, Train Acc: 99.2973% | Test loss: 2.9123, Test Acc: 57.7303%\n",
      "Epoch: 73 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0262, Train Acc: 99.1679% | Test loss: 3.1342, Test Acc: 57.8947%\n",
      "Epoch: 74 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0038, Train Acc: 99.8891% | Test loss: 4.0511, Test Acc: 58.7171%\n",
      "Epoch: 75 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0047, Train Acc: 99.8891% | Test loss: 3.9756, Test Acc: 58.7171%\n",
      "Epoch: 76 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0030, Train Acc: 99.9630% | Test loss: 4.3534, Test Acc: 58.2237%\n",
      "Epoch: 77 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0009, Train Acc: 99.9815% | Test loss: 4.5914, Test Acc: 58.8816%\n",
      "Epoch: 78 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0003, Train Acc: 100.0000% | Test loss: 4.4515, Test Acc: 60.3618%\n",
      "Epoch: 79 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0001, Train Acc: 100.0000% | Test loss: 4.6320, Test Acc: 60.0329%\n",
      "Epoch: 80 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.7376, Test Acc: 59.8684%\n",
      "Epoch: 81 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8042, Test Acc: 59.8684%\n",
      "Epoch: 82 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.8607, Test Acc: 59.8684%\n",
      "Epoch: 83 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9130, Test Acc: 59.8684%\n",
      "Epoch: 84 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 4.9617, Test Acc: 59.7039%\n",
      "Epoch: 85 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0078, Test Acc: 59.7039%\n",
      "Epoch: 86 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0522, Test Acc: 59.7039%\n",
      "Epoch: 87 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.0951, Test Acc: 59.7039%\n",
      "Epoch: 88 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1367, Test Acc: 59.5395%\n",
      "Epoch: 89 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.1772, Test Acc: 59.5395%\n",
      "Epoch: 90 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2168, Test Acc: 59.5395%\n",
      "Epoch: 91 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2557, Test Acc: 59.5395%\n",
      "Epoch: 92 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.2942, Test Acc: 59.5395%\n",
      "Epoch: 93 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.3321, Test Acc: 59.5395%\n",
      "Epoch: 94 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.3699, Test Acc: 59.5395%\n",
      "Epoch: 95 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.4073, Test Acc: 59.5395%\n",
      "Epoch: 96 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.4442, Test Acc: 59.5395%\n",
      "Epoch: 97 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.4809, Test Acc: 59.5395%\n",
      "Epoch: 98 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.5174, Test Acc: 59.5395%\n",
      "Epoch: 99 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0000, Train Acc: 100.0000% | Test loss: 5.5538, Test Acc: 59.5395%\n"
     ]
    }
   ],
   "source": [
    "train(bert_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd0175-5a5b-470c-90f3-2b794f8f0f32",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778814c5-c481-4ced-992a-66e36c98b5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = WiCDataset(train_path, \"gpt\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "valid_data = WiCDataset(valid_path, \"gpt\")\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)\n",
    "test_data = WiCDataset(test_path, \"gpt\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751de384-aad0-4c59-9670-32b919478508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_model = DNN(input_size = 49920, hidden_size=512, num_classes=1).to(device)\n",
    "#gpt_model.load_state_dict(torch.load(\"gpt_model_wic_1.pth\"))\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "optimizer = torch.optim.Adam(gpt_model.parameters(), lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874f300-966d-4062-bb43-cfe233fe61f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5a499f-8654-44ed-9edf-cbc64060bf08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.7317, Train Acc: 55.5658% | Test loss: 0.6924, Test Acc: 52.9605%\n",
      "Epoch: 1 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.6588, Train Acc: 62.1487% | Test loss: 0.7037, Test Acc: 54.4408%\n",
      "Epoch: 2 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5954, Train Acc: 69.0089% | Test loss: 0.7187, Test Acc: 54.4408%\n",
      "Epoch: 3 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.5058, Train Acc: 74.8706% | Test loss: 0.7777, Test Acc: 54.9342%\n",
      "Epoch: 4 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4365, Train Acc: 79.4564% | Test loss: 0.8559, Test Acc: 53.7829%\n",
      "Epoch: 5 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.4179, Train Acc: 80.6953% | Test loss: 0.7232, Test Acc: 56.5789%\n",
      "Epoch: 6 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3550, Train Acc: 84.1161% | Test loss: 0.8076, Test Acc: 56.9079%\n",
      "Epoch: 7 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3540, Train Acc: 84.2825% | Test loss: 1.1250, Test Acc: 53.2895%\n",
      "Epoch: 8 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3262, Train Acc: 85.6694% | Test loss: 1.0088, Test Acc: 58.5526%\n",
      "Epoch: 9 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2521, Train Acc: 89.4970% | Test loss: 1.2806, Test Acc: 57.2368%\n",
      "Epoch: 10 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2892, Train Acc: 86.1132% | Test loss: 0.9695, Test Acc: 52.1382%\n",
      "Epoch: 11 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3381, Train Acc: 83.7648% | Test loss: 0.7535, Test Acc: 54.9342%\n",
      "Epoch: 12 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2714, Train Acc: 89.0163% | Test loss: 1.1324, Test Acc: 59.8684%\n",
      "Epoch: 13 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1999, Train Acc: 92.5481% | Test loss: 1.1866, Test Acc: 59.3750%\n",
      "Epoch: 14 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1739, Train Acc: 93.6021% | Test loss: 1.3137, Test Acc: 58.0592%\n",
      "Epoch: 15 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1791, Train Acc: 93.2507% | Test loss: 1.0508, Test Acc: 55.2632%\n",
      "Epoch: 16 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1896, Train Acc: 92.2152% | Test loss: 0.9828, Test Acc: 50.6579%\n",
      "Epoch: 17 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.3711, Train Acc: 85.4475% | Test loss: 1.0113, Test Acc: 50.6579%\n",
      "Epoch: 18 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2155, Train Acc: 89.6450% | Test loss: 1.4515, Test Acc: 56.0855%\n",
      "Epoch: 19 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1963, Train Acc: 92.1043% | Test loss: 1.4298, Test Acc: 56.0855%\n",
      "Epoch: 20 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2777, Train Acc: 89.7929% | Test loss: 1.1689, Test Acc: 56.2500%\n",
      "Epoch: 21 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2639, Train Acc: 90.0888% | Test loss: 1.4137, Test Acc: 58.7171%\n",
      "Epoch: 22 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1755, Train Acc: 93.4911% | Test loss: 1.8134, Test Acc: 58.2237%\n",
      "Epoch: 23 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1157, Train Acc: 95.9135% | Test loss: 2.0593, Test Acc: 57.2368%\n",
      "Epoch: 24 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0998, Train Acc: 96.5607% | Test loss: 2.4218, Test Acc: 56.0855%\n",
      "Epoch: 25 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1104, Train Acc: 95.9320% | Test loss: 2.4055, Test Acc: 55.5921%\n",
      "Epoch: 26 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0674, Train Acc: 97.4112% | Test loss: 3.0539, Test Acc: 56.5789%\n",
      "Epoch: 27 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0508, Train Acc: 98.3728% | Test loss: 3.7849, Test Acc: 53.4539%\n",
      "Epoch: 28 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.0633, Train Acc: 97.6886% | Test loss: 4.0415, Test Acc: 52.4671%\n",
      "Epoch: 29 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1491, Train Acc: 95.0999% | Test loss: 2.5731, Test Acc: 54.4408%\n",
      "Epoch: 30 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.2603, Train Acc: 89.8299% | Test loss: 1.6892, Test Acc: 56.9079%\n",
      "Epoch: 31 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1517, Train Acc: 94.3602% | Test loss: 4.2876, Test Acc: 52.3026%\n",
      "Epoch: 32 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1406, Train Acc: 94.9519% | Test loss: 3.0756, Test Acc: 54.9342%\n",
      "Epoch: 33 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1623, Train Acc: 94.0459% | Test loss: 2.1769, Test Acc: 55.0987%\n",
      "Epoch: 34 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1870, Train Acc: 93.8240% | Test loss: 1.8780, Test Acc: 57.0724%\n",
      "Epoch: 35 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1498, Train Acc: 94.2678% | Test loss: 3.0649, Test Acc: 54.1118%\n",
      "Epoch: 36 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1466, Train Acc: 94.3047% | Test loss: 2.9924, Test Acc: 54.1118%\n",
      "Epoch: 37 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1405, Train Acc: 94.6006% | Test loss: 3.4735, Test Acc: 54.7697%\n",
      "Epoch: 38 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1955, Train Acc: 92.1967% | Test loss: 3.2447, Test Acc: 56.0855%\n",
      "Epoch: 39 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1556, Train Acc: 94.0089% | Test loss: 4.2185, Test Acc: 52.1382%\n",
      "Epoch: 40 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1203, Train Acc: 95.5067% | Test loss: 2.9775, Test Acc: 54.1118%\n",
      "Epoch: 41 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1194, Train Acc: 95.5436% | Test loss: 3.2408, Test Acc: 55.9211%\n",
      "Epoch: 42 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1235, Train Acc: 95.8950% | Test loss: 2.6156, Test Acc: 55.0987%\n",
      "Epoch: 43 \n",
      " ==========\n",
      "\n",
      "Train loss: 0.1028, Train Acc: 96.6346% | Test loss: 2.8477, Test Acc: 57.2368%\n",
      "Epoch: 44 \n",
      " ==========\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_loader, valid_loader, num_epochs, device, accuracy_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m### Training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     flattened_inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mWiCDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     31\u001b[0m    \u001b[38;5;66;03m# self.tokenizer.pad_token = self.tokenizer.eos_token\u001b[39;00m\n\u001b[0;32m     32\u001b[0m    \u001b[38;5;66;03m# gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     gpt_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJoined\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idx], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 34\u001b[0m     gpt_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_token\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m     padded_outputs \u001b[38;5;241m=\u001b[39m pad_sequences(gpt_outputs)\n\u001b[0;32m     36\u001b[0m    \u001b[38;5;66;03m# with torch.inference_mode():\u001b[39;00m\n\u001b[0;32m     37\u001b[0m    \u001b[38;5;66;03m#     gpt_outputs = self.model(**gpt_token)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    387\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 389\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    398\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:182\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 182\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[0;32m    185\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[0;32m    186\u001b[0m             [], value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    187\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "train(gpt_model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    accuracy_fn,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1bf40f0-5155-416e-adb7-8274583ceadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (fc1): Linear(in_features=49920, out_features=512, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9a2057-c34b-45bb-abc5-be08fe73ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gpt_model.state_dict(), 'gpt_model_wic_2.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a72ee17-0e07-4183-a70e-d4bb006f17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(gpt_model.state_dict(), \"gpt_model_wic_1.pth\")\n",
    "# torch.save(bert_model.state_dict(), \"bert_model_wic_1.pth\")\n",
    "# torch.save(glove_model.state_dict(), \"glove_model_wic_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e7b94-73a1-441a-b7be-77446c79832a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b501fc8-84c1-4fea-a32c-0476d5e95eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751d2ce-1dc9-400a-a779-49853301c394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd46cb7-d8b1-43a8-8a8d-613e87bc0eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c815463-6a45-4243-a4fb-b01c3e4b1343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660e64c-2729-4e59-b298-6a4ffabfa1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0205425-721b-49bc-8ea5-30dd3484eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55c854-5289-4633-8174-de82c24a2d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56ac2d-3138-4631-bb87-eb7f00ffac56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b0305-c7c0-4b83-86ea-3d29c292b180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae60a0-1e13-431a-8afe-804fed975a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3a9db-f77f-419d-95b0-a7668786e9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbf0b6-0345-4c05-bf0a-2e96221e9104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4f563-0e95-437c-a33a-484fb145aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfabf5-8623-4c57-ba9d-3c0f61c6be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0006565-394d-47cd-8671-76ddf35c7c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b390560-a637-43b7-a0e2-d615d7454fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48da91f-79af-422e-b60d-eb955e542666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0908c-2b5c-45dc-a60e-9899c8d1bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a7914-be59-4fb4-8959-2355868fb9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dcfb3-aab2-438e-bfc7-461f16861219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16add5-7815-4dd7-9205-958c5d1189a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710d66-f558-4ec8-97a8-ba7046298dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04c418-0ebb-4170-bde8-f228b26fb21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c32b-ac46-4b3c-9500-8ca06ef925a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6854c1a-0236-4018-bfea-55420d60a379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217901fa-14b9-4f71-8d51-b96441c9880b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b643562-3521-4c0f-823b-bf24f4c5dbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594bd16-c7b4-4535-8d13-eb04f9f1525c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a3dab-119e-4dd3-bfe5-390ed6a669ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab84bb6-57d0-4506-855d-c839957d1a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b662c6-7348-44b6-b24c-645536c36795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53332a-f7e5-4acd-9674-d103b4b2743a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589595c7-71a6-4105-95fa-35863038b692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cc0a9-8570-4fa3-b15e-e14d8f524efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255e536-46ef-4df3-9ba3-e726489a572b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b39863-47c9-4aea-ad69-ded3dcb3caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee083b-67c2-404a-a565-4aa86418ad4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e04e2-1227-425f-8684-e3bc21e1c436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5eb14-2438-438b-b9fb-414e342f3fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d7349-3aaf-40ab-b9bf-e0d910e94dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5e2e0-9383-41c5-8607-68928c536e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2fa89-9303-4fa7-b909-93d024d5bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f817c-6a46-40e2-9823-e853cdb996fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838b94a-69bd-46b4-a4c6-977a52f96d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa210325-24e6-4d6c-bac3-c233ad3c1b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c51cf1-6394-4735-a954-fa935fd74db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56d606-8a88-4417-a165-1760ec129518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937015e-2c4d-4226-9f39-c19aab33b949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e9415-66fb-4f6b-89ae-83ce80fe844b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e1a31-2a35-4831-8356-a5ee0a8c8d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dcfa6-5b1a-4d3e-a400-0771d04484b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344b8ad-a2aa-4bb8-8ae3-44bf3d4ae781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
