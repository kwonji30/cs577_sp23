{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cfcc7-0c56-470f-a717-c27d9451e614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "glove = api.load('glove-wiki-gigaword-50')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf4646d-0bbf-4593-b992-1f256d9e37bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) <= 65:\n",
    "            padded_seq = torch.nn.functional.pad(seq, (0, 0, 0, 65 - seq.size(0)), mode='constant', value=0)\n",
    "        else:\n",
    "            padded_seq = seq[:65]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67474f6-1d61-4991-9274-957c52c2fbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "class WiCDataset(Dataset):\n",
    "    def __init__(self, path, mode):\n",
    "        self.mode = mode\n",
    "        if mode == \"gpt\":\n",
    "            self.mode = 'gpt2'\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "        elif mode == \"bert\":\n",
    "            self.mode = 'bert-base-uncased'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.mode) \n",
    "            self.model = AutoModel.from_pretrained(self.mode).to(device)\n",
    "\n",
    "        df_data = pd.read_csv(path+\"data.txt\",\n",
    "                              delimiter='\\t',\n",
    "                              names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "        df_label = pd.read_csv(path+'gold.txt',\n",
    "                               delimiter='\\t',\n",
    "                               names=['label'])\n",
    "        self.data = pd.concat([df_data, df_label], axis=1)\n",
    "        self.data['Joined'] = self.data['Context1'] + \" \" + self.data['Context2']\n",
    "        self.data['label'] = self.data['label'].map(lambda x: 0 if x == 'F' else 1)\n",
    "        #self.maxLength = find_maxLength(self.data['Joined'].tolist(), self.tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'gpt2':\n",
    "           # self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "           # gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_token = self.tokenizer(self.data['Joined'].iloc[idx], return_tensors='pt').to(device)\n",
    "            gpt_outputs = self.model(gpt_token['input_ids'])[0]\n",
    "            padded_outputs = pad_sequences(gpt_outputs)\n",
    "           # with torch.inference_mode():\n",
    "           #     gpt_outputs = self.model(**gpt_token)\n",
    "            return (padded_outputs, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)) \n",
    "            \n",
    "            # sentence_lists = chunk_list(self.data['Joined'].tolist(), 200)\n",
    "            # tensor_list = []\n",
    "            # for block in sentence_lists:\n",
    "            #     torch.cuda.empty_cache()\n",
    "            #     gpt_token = self.tokenizer(block, padding='max_length', return_tensors='pt', max_length=65).to(device)\n",
    "            #     with torch.inference_mode():\n",
    "            #         gpt_outputs = self.model(**gpt_token)\n",
    "            #     tensor_list.append(gpt_outputs[0])\n",
    "            # gpt_tensor = torch.cat(tensor_list, dim = 0)\n",
    "            # return (gpt_tensor[idx].cpu(), torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long))\n",
    "        \n",
    "        \n",
    "        elif self.mode == 'bert-base-uncased':\n",
    "            bert_token = self.tokenizer(self.data['Joined'].iloc[idx], padding='max_length', return_tensors='pt', max_length=68).to(device)        \n",
    "            with torch.inference_mode():\n",
    "                bert_outputs = self.model(**bert_token)\n",
    "            return (bert_outputs[0], torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32))\n",
    "        \n",
    "            \n",
    "        elif self.mode == 'glove':\n",
    "            row = self.data.iloc[idx]\n",
    "            words = word_tokenize(row.Joined.lower())\n",
    "\n",
    "            indices = [glove.get_index(w) for w in words if glove.has_index_for(w)]\n",
    "            indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "            return indices_tensor, torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "train_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\train\\train.\"\n",
    "valid_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\dev\\dev.\"\n",
    "test_path = r\"C:\\Users\\joowa\\OneDrive\\Spring 2023\\CS577\\Project\\WiC_dataset\\test\\test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329c5e92-d439-4382-adcf-af67e0689c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc97bfb1-bd47-4e7f-9d7c-44980c694347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.FloatTensor(glove.vectors))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers,\n",
    "                          bidirectional = True,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, seq, seq_length):\n",
    "        inputs_embedded = self.emb(seq)\n",
    "        seq_length = seq_length.cpu()\n",
    "        packed_input = rnn_utils.pack_padded_sequence(inputs_embedded, seq_length, batch_first=True)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), seq_length - 1, :self.hidden_dim]\n",
    "        out_reverse = output[:, 0, self.hidden_dim:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        output = self.fc(out_reduced)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be60728d-4f07-4e91-b4e6-894bee121d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    # pad the inputs with zeros to make them the same length\n",
    "    inputs_padded = rnn_utils.pad_sequence(inputs, batch_first=True)\n",
    "    # get the sequence lenghts of the inputs\n",
    "    seq_length = torch.LongTensor([len(seq) for seq in inputs])\n",
    "    \n",
    "    # sort the inputs and labels by the sequence lengths\n",
    "    seq_length, sort_idx = seq_length.sort(descending=True)\n",
    "    inputs_padded = inputs_padded[sort_idx].to(device)\n",
    "    labels_sorted = torch.tensor(labels, dtype=torch.float32)[sort_idx].to(device)\n",
    "\n",
    "    return inputs_padded, labels_sorted, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8d011c-e1b3-4439-9be7-f54ec23ab4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Make each dataloader\n",
    "# glove_data = WiCDataset(train_path, \"glove\")\n",
    "# glove_dataloader = torch.utils.data.DataLoader(glove_data,\n",
    "#                                               batch_size=32,\n",
    "#                                               shuffle = False, \n",
    "#                                               drop_last=False,\n",
    "#                                               collate_fn=collate_fn)\n",
    "bert_data = WiCDataset(train_path, \"bert\")\n",
    "bert_dataloader = torch.utils.data.DataLoader(bert_data,\n",
    "                                             batch_size=32,\n",
    "                                             shuffle=False,\n",
    "                                             drop_last=False)\n",
    "gpt_data = WiCDataset(train_path, \"gpt\")\n",
    "gpt_dataloader = torch.utils.data.DataLoader(gpt_data,\n",
    "                                            batch_size=32,\n",
    "                                            shuffle = False,\n",
    "                                            drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8287fc79-5104-42ef-acff-a225a498c7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the models\n",
    "# glove_model = LSTM(50, 128, 1, 2).to(device)\n",
    "# glove_model.load_state_dict(torch.load(\"glove_model_wic_1.pth\"))\n",
    "bert_model = DNN(input_size = 52224, hidden_size=512, num_classes=1).to(device)\n",
    "bert_model.load_state_dict(torch.load(\"bert_model_wic_1.pth\"))\n",
    "gpt_model = DNN(input_size = 49920, hidden_size=512, num_classes=1).to(device)\n",
    "gpt_model.load_state_dict(torch.load(\"gpt_model_wic_2.path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33476cb6-f96b-410f-b9f8-45754902dc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(dataloader, model, mode, device):\n",
    "    result = []\n",
    "    if mode == \"glove\":\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, _, seq_lengths in dataloader:\n",
    "                inputs, seq_lengths = inputs.to(device), seq_lengths.to(device)\n",
    "                test_logits = model(inputs, seq_lengths)\n",
    "                result.append(torch.sigmoid(test_logits))\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for inputs, _ in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                flattened_inputs = inputs.view(inputs.size(0), -1)\n",
    "                test_logits = model(flattened_inputs)\n",
    "                result.append(torch.sigmoid(test_logits))\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e3239d-e906-434e-8ccd-9db07760c23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_output = predict(gpt_dataloader, gpt_model, \"gpt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70dfbd8d-9e45-4082-9786-db7dccee04d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_output = predict(bert_dataloader, bert_model, \"bert\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa9fdbc-ecca-4c8e-8498-64bc01828449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# glove_output = predict(glove_dataloader, glove_model, \"glove\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5358dd0c-14ea-442c-bf69-1038bb5a6864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_vector = torch.cat(gpt_output, dim=0).squeeze()\n",
    "bert_vector = torch.cat(bert_output, dim=0).squeeze()\n",
    "# glove_vector = torch.cat(glove_output, dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ca35be-6df6-4a01-b8cb-df59585af55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average_vector = torch.mean(torch.stack([gpt_vector, bert_vector, glove_vector]), dim=0).cpu().numpy()\n",
    "average_vector = torch.mean(torch.stack([gpt_vector, bert_vector]), dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac450a9f-68df-47ea-8f71-4edbbbac07f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8690046e-03, 2.3604396e-05, 3.6915534e-08, ..., 6.9479299e-01,\n",
       "       5.9548412e-02, 9.9967980e-01], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1926df93-fec2-44be-a8bc-d2196b0d1134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_path+'data.txt',\n",
    "                delimiter='\\t',\n",
    "                names=['Target Word', 'PoS', 'Index', 'Context1', 'Context2'])\n",
    "df['Joined'] = df['Context1'] + \" \" + df['Context2']\n",
    "df = df.assign(Label=average_vector)\n",
    "df = df.drop(['PoS', 'Index', 'Context1', 'Context2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e69f68-980c-492d-a3b1-545495f3bc70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Word</th>\n",
       "      <th>Joined</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carry</td>\n",
       "      <td>You must carry your camping gear . Sound carri...</td>\n",
       "      <td>6.869005e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>Messages must go through diplomatic channels ....</td>\n",
       "      <td>2.360440e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>break</td>\n",
       "      <td>Break an alibi . The wholesaler broke the cont...</td>\n",
       "      <td>3.691553e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cup</td>\n",
       "      <td>He wore a jock strap with a metal cup . Bees f...</td>\n",
       "      <td>9.999992e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academy</td>\n",
       "      <td>The Academy of Music . The French Academy .</td>\n",
       "      <td>4.864547e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>krona</td>\n",
       "      <td>Piecas kronas — five krona . Kronas kurss — th...</td>\n",
       "      <td>9.999934e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>conflict</td>\n",
       "      <td>The harder the conflict the more glorious the ...</td>\n",
       "      <td>3.771217e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>answer</td>\n",
       "      <td>Answer the riddle . Answer a question .</td>\n",
       "      <td>6.947930e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>play</td>\n",
       "      <td>Play the casinos in Trouville . Play the races .</td>\n",
       "      <td>5.954841e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>invasion</td>\n",
       "      <td>An invasion of bees . An invasion of mobile ph...</td>\n",
       "      <td>9.996798e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5428 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target Word                                             Joined   \n",
       "0          carry  You must carry your camping gear . Sound carri...  \\\n",
       "1             go  Messages must go through diplomatic channels ....   \n",
       "2          break  Break an alibi . The wholesaler broke the cont...   \n",
       "3            cup  He wore a jock strap with a metal cup . Bees f...   \n",
       "4        academy        The Academy of Music . The French Academy .   \n",
       "...          ...                                                ...   \n",
       "5423       krona  Piecas kronas — five krona . Kronas kurss — th...   \n",
       "5424    conflict  The harder the conflict the more glorious the ...   \n",
       "5425      answer            Answer the riddle . Answer a question .   \n",
       "5426        play   Play the casinos in Trouville . Play the races .   \n",
       "5427    invasion  An invasion of bees . An invasion of mobile ph...   \n",
       "\n",
       "             Label  \n",
       "0     6.869005e-03  \n",
       "1     2.360440e-05  \n",
       "2     3.691553e-08  \n",
       "3     9.999992e-01  \n",
       "4     4.864547e-09  \n",
       "...            ...  \n",
       "5423  9.999934e-01  \n",
       "5424  3.771217e-02  \n",
       "5425  6.947930e-01  \n",
       "5426  5.954841e-02  \n",
       "5427  9.996798e-01  \n",
       "\n",
       "[5428 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67dfbae-a787-47a0-95f2-78fa153786f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266ca8f-af2a-4779-8581-c7998bccd2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78f934-5897-4294-8a78-67a1ec7f7382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17400ee5-7cbc-4bac-a4d2-48f8562196bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
